<div align="center" markdown>

# [Atelier Scraping](https://github.com/Mastocodeur/Tutorial_Scraping/)

Atelier pour d√©couvrir deux modules python de scraping : Selenium et BeautifulSoup.

<div style="text-align: center;">
    <img src="activities_in_french/images/web_scraping.png" width="500" height="300">
</div>

üåè
[**Anglais**](https://github.com/Mastocodeur/tuto_scraping/blob/main/README.md) |
Fran√ßais |</div>


## Table des Mati√®res

1. [Qu'est-ce que le scraping ?](#le-scraping-quest-ce-que-cest-)
2. [L√©gislation autour du scraping](#l√©gislation-autour-du-scraping)
3. [Selenium vs BeautifulSoup](#selenium-vs-beautifulsoup)
4. [Les √©volutions de S√©l√©nium](#les-√©volutions-de-s√©l√©nium)
5. [Introduction au HTML](#introduction-au-html)
6. [Les diff√©rentes options du driver](#les-diff√©rentes-options-dun-driver)
7. [Les diff√©rentes fa√ßons de localiser des √©l√©ments Web](#les-diff√©rentes-fa√ßon-de-localiser-des-√©l√©ments-web)
8. [Les parsers de BeautifulSoup](#les-parsers-de-beautifulsoup)
9. [Avenir du scraping : LaVague](#avenir-du-scraping--lavague)
10. [Crawling et Robots.txt](#crawling-et-robotstxt)
11. [Sources et documentation](#sources-et-documentation)


## Le scraping, qu'est-ce que c'est ?

Le scraping, ou "web scraping", est une technique utilis√©e pour extraire automatiquement des donn√©es √† partir de sites web. Cette m√©thode permet de r√©cup√©rer des informations structur√©es ou semi-structur√©es disponibles sur des pages web, que l'on peut ensuite analyser, traiter ou stocker pour divers usages. 

Le scraping permet √©galement d'automatiser des t√¢ches sur un site. 

Il existe plusieurs module python permettant de faire du scraping : `BeautifulSoup`, `Selenium`, `Scrapy`, `Requests`, `Octoparse`, etc.

Dans cet atelier, nous exploiterons `BeautifulSoup` en combinaison avec `Requests`, et `Selenium` au cours des diff√©rents modules. Ces deux outils offrent une vaste gamme de fonctionnalit√©s pour le scraping et seront donc de v√©ritables portes d'entr√©e vers ce domaine.

Pour les projets de grande envergure, on utilise souvent des frameworks comme `Scrapy`, qui permettent de r√©partir le travail de scraping sur plusieurs machines et de g√©rer de grandes quantit√©s de donn√©es.

## L√©gislation autour du scraping

Le scraping web est g√©n√©ralement autoris√© dans les cas suivants :
- Les donn√©es extraites sont des donn√©es accessibles au public.
- Les informations recueillies ne sont pas prot√©g√©es par un login.

Bien que la technique elle-m√™me ne soit pas ill√©gale, son utilisation peut enfreindre les conditions d'utilisation de certains sites.

Il est important d'√™tre prudent lorsque l'on se lance dans une t√¢che de scraping, notamment vis-√†-vis des termes de service, des donn√©es prot√©g√©es par le droit d'auteur et des donn√©es personnelles. Les conditions d'utilisation des sites doivent √™tre respect√©es, car les donn√©es personnelles sont g√©n√©ralement prot√©g√©es par les lois sur la protection des donn√©es.

Il faut √©galement prendre en compte l'impact sur les serveurs, notamment en cas de scraping intensif. Il est recommand√© d'utiliser des techniques comme le "rate limiting" pour limiter le nombre de requ√™tes envoy√©es en un temps donn√©.

En droit de la concurrence, le web scraping peut √™tre qualifi√© d'acte de concurrence d√©loyale. 

La CNIL a par exemple condamn√© la soci√©t√© Nestor √† une amende de 20000 euros car elle avait construit sa base de prospects en ayant recours √† la pratique de web scraping √† partir de donn√©es accessibles sur le r√©seau social professionnel Linkedin (pour plus d'informations, voir les articles mentionn√©s dans la partie [Sources et documentation](#sources-et-documentation)).

## Selenium vs BeautifulSoup

<div align="center">
    <img src="activities_in_french/images/selenium_vs_beautifulsoup.jpg" width="1000" height="300">
</div>


**Selenium** est utile pour les pages web dynamiques o√π le contenu est g√©n√©r√© via JavaScript, n√©cessitant des interactions utilisateur telles que les clics, le d√©filement, ou la saisie de texte.

**BeautifulSoup** est une biblioth√®que Python utilis√©e principalement pour parser (analyser) des documents HTML et XML. Elle est utile pour extraire des donn√©es structur√©es √† partir de pages web statiques.

## Les √©volutions de S√©l√©nium

Le module S√©l√©nium est probablement l'un des modules python qui a connu le plus de modification au cours du temps. 
Ces modifications ont chang√© la mani√®re de l'utiliser. Il est important d'avoir une vision globale de ces √©volutions pour comprendre les diff√©rentes probl√©matiques qu'a connu ce module et aussi pouvoir comprendre les diff√©rents forums et codes qui circulent sur internet. 

### Selenium 1.x (avant 2011)

La premi√®re version de S√©l√©nium n√©cessitait un serveur pour interagir avec les navigateurs. La gestion des drivers n'√©tait pas encore une pr√©occupation majeure, car Selenium RC interagissait directement avec les navigateurs via JavaScript.

### Selenium 2.x (2011-2016)

Dans cette version, on introduit Selenium WebDriver. Cette version marquait un changement significatif en permettant une interaction directe avec les navigateurs √† travers des API sp√©cifiques appel√©es "drivers". Chaque navigateur (Chrome, Firefox, etc.) n√©cessitait un driver sp√©cifique pour fonctionner.

Le code pour utiliser un driver et donc se connecter √† un site ressemblait √† √ßa : 

    driver = webdriver.Chrome(executable_path='/path/to/chromedriver')

### Selenium 3.x (2016-2021)

Les utilisateurs devaient encore t√©l√©charger manuellement les drivers et d√©finir le chemin dans le code. 
 
Cependant, des outils tiers comme `webdriver-manager` ou `chromedriver-autoinstaller` ont commenc√© √† √©merger, facilitant la gestion automatique des drivers.

Vers la fin de cette p√©riode (version 3.141.0 notamment), des biblioth√®ques comme `WebDriverManager` en Java ou `webdriver_manager` en Python ont commenc√© √† gagner en popularit√©. Elles permettaient de t√©l√©charger automatiquement le bon driver sans que l'utilisateur ait √† g√©rer manuellement les chemins d'acc√®s :

    from webdriver_manager.chrome import ChromeDriverManager
    driver = webdriver.Chrome(ChromeDriverManager().install())

### Selenium 4.x (d√®s 2021)

Selenium 4 a apport√© de nombreuses am√©liorations, y compris une API mise √† jour et des fonctionnalit√©s pour les tests sans serveur.

La version 4.6.0 de Selenium, publi√©e en octobre 2022, a introduit une fonctionnalit√© majeure qui automatise enti√®rement la gestion des drivers. Selenium t√©l√©charge et configure automatiquement le bon driver pour le navigateur sp√©cifi√©, sans besoin d'une biblioth√®que externe. Le code devient : 

    from selenium import webdriver
    driver = webdriver.Chrome()

Pour suivre les √©volutions de ce module python et rester √† jour sur son scraping : [Les diff√©rentes mises √† jour de Selenium](https://github.com/SeleniumHQ/selenium/blob/trunk/py/CHANGES)

## Introduction au HTML
Scraper n√©cessite de savoir interpr√©ter les codes sources des pages web, et donc comprendre le HTML.

Voici un petit [Formulaire HTML](activities_in_french/formulaire_html.md). Il permet rapidemment d'apprendre ou se rem√©morer les balises principales permettant de lire du HTML et d'ainsi rep√©rer les diff√©rents √©l√©ments d'une page web.


## Les diff√©rentes options d'un driver

Quand on utilise Selenium, il est important de connaitre les options du driver.

Voici un formulaire qui pr√©sente un √©tat de l'art des principales options d'un driver : [Formulaire Driver](activities_in_french/formulaire_options.md)

## Les diff√©rentes fa√ßon de localiser des √©l√©ments Web 

Pour interagir avec des √©l√©ments sur une page web, il est essentiel de savoir comment les localiser. Il existe plusieurs m√©thodes pour identifier ces √©l√©ments dans le DOM (Document Object Model) d'une page web :

- **ID** :  L'attribut `id` est unique pour chaque √©l√©ment, ce qui en fait un moyen rapide et pr√©cis de localiser un √©l√©ment.

- **NAME** : L'attribut `name` est souvent utilis√© dans les formulaires et permet de cibler des √©l√©ments bas√©s sur leur nom.

- **XPATH** :  `XPath` est un langage qui permet de naviguer dans la structure du document XML/HTML pour trouver un √©l√©ment selon son emplacement relatif dans l'arbre DOM.

- **LINK_TEXT** : Cette m√©thode localise un lien (balise `<a>`) en fonction du texte visible qu'il contient.

- **PARTIAL_LINK_TEXT** : Similaire √† LINK_TEXT, mais permet de localiser un lien en utilisant une partie seulement du texte visible.

- **TAG_NAME** : Permet de cibler les √©l√©ments par leur nom de balise HTML, comme `<div>`, `<input>`, etc.

- **CLASS_NAME** : Cette m√©thode localise les √©l√©ments en fonction de la valeur de leur attribut `class`, utile pour cibler des √©l√©ments avec un style ou une fonctionnalit√© sp√©cifique.

- **CSS_SELECTOR** : Un s√©lecteur CSS permet de cibler des √©l√©ments en utilisant des r√®gles similaires √† celles utilis√©es en CSS pour le style des √©l√©ments.

Chacune de ces m√©thodes offre un niveau de pr√©cision et de flexibilit√© diff√©rent, en fonction du contexte et de la structure de la page web √† analyser.

Pr√©cision : **XPATH** et **CSS_SELECTOR** sont deux options qui permettent de cibler un ou plusieurs √©l√©ments avec plusieurs classes.

## Les parsers de BeautifulSoup

Un parser est un composant ou un outil qui analyse un document HTML ou XML et le d√©compose en une structure que BeautifulSoup peut manipuler et interroger. Concr√®tement, BeautifulSoup utilise un parser pour lire le contenu d'une page web (HTML/XML) et le convertir en un arbre d'objets Python, permettant ainsi d'extraire et de manipuler facilement des donn√©es sp√©cifiques √† partir du document.

Voici quelques types de parsers que vous pouvez utiliser avec BeautifulSoup :

- `html.parser` : C'est le parser HTML int√©gr√© √† Python. Il est rapide, ne n√©cessite pas d'installation suppl√©mentaire. Cependant, il est moins rapide que `lxml` et moins tol√©rant qu'`html5lib`.

- `lxml` : Un parser externe qui doit √™tre install√© s√©par√©ment. Il est tr√®s rapide et tol√©rant aux erreurs, capable de traiter √† la fois le HTML et le XML. Il est souvent pr√©f√©r√© pour sa performance. Son inconv√©nient principal est qu'il n√©cessite une d√©pendance externe en C.

- `lxml-xml` : C'est un parser tr√®s rapide, et le seul actuellement support√© pour XML. Comme le parser HTML de `lxml`, il n√©cessite √©galement une d√©pendance externe en C.

- `html5lib` : Ce parser est plus complet et conforme aux standards du HTML5. Il analyse les documents de mani√®re plus proche de la fa√ßon dont un navigateur le ferait, mais il est plus lent et n√©cessite √©galement une installation suppl√©mentaire. Son principal inconv√©nient est sa lenteur, en plus de n√©cessiter une d√©pendance externe en Python.


## Avenir du scraping : LaVague

Apr√®s avoir suivi les √©volutions de Selenium ces derni√®res ann√©es, il est l√©gitime de se demander quel sera l'avenir du scraping.

L'une des grandes probl√©matiques du scraping r√©side dans l'√©volution constante des sites internet. En effet, les programmes Python utilis√©s pour le scraping sont fragiles, car la structure des sites web peut changer fr√©quemment, que ce soit par des modifications des noms de balises, des classes CSS, ou d'autres √©l√©ments.

L'√©mergence de l'intelligence artificielle joue alors un r√¥le cl√© dans l'√©volution du scraping.

C'est dans cette perspective que s'inscrit le module [lavague.ai](https://github.com/lavague-ai).


<div align = "center">
  <img src="activities_in_french/images/lavague.png" alt="Description de l'image" />
</div>


LaVague est une technologie de scraping de nouvelle g√©n√©ration qui combine des techniques avanc√©es d'apprentissage automatique avec des m√©thodes traditionnelles de scraping. Elle permet une extraction de donn√©es plus intelligente, en tenant compte non seulement de la structure des pages web, mais aussi du contexte et de la s√©mantique des informations qu'elles contiennent.

C'est entre autres l'une des solutions qui, √† terme, permettra de cr√©er des scripts beaucoup plus robustes face aux √©volutions des diff√©rentes pages web √† scraper.

LaVague permet ainsi une adaptabilit√© en temps r√©el aux changements de structure des sites web.

Ce module permet √©galement une automatisation pouss√©e des t√¢ches de scraping, avec des workflows personnalisables qui peuvent √™tre ajust√©s en fonction des besoins sp√©cifiques de l'utilisateur.

L'un des cas les plus concrets est pr√©sent√© ici : [Remplir des formulaires](https://docs.lavague.ai/en/latest/docs/use-cases/forms/)


## Crawling et Robots.txt

Cette derni√®re partie est une ouverture. Elle permet d'aborder la notion de crawling et d'√©tablir une distinction claire entre crawling et scraping.
### Crawling
Le crawling ou indexation naturelle d√©signe le processus que les grands moteurs de recherche comme Google entreprennent lorsqu‚Äôils envoient leurs robots d'indexation, comme Googlebot, parcourir le r√©seau pour indexer le contenu Internet. 

Voici quelques diff√©rences notables :

- Le scraper se fait pour un navigateur web tandis qu'un robot d'indexation indique clairement son intenton et ne cherche pas √† masquer son identit√©.

- Les scrapers peuvent effectuer des actions avanc√©es, comme remplir des formulaires ou adopter un comportement permettant d'acc√©der √† certaines parties d‚Äôun site web, ce que les robots d'indexation ne font g√©n√©ralement pas.

- Les scrapers ignorent souvent le fichier robots.txt, con√ßu pour guider les robots d'indexation sur les donn√©es √† analyser et les zones du site √† √©viter. Contrairement aux robots d'indexation, ces extracteurs ciblent d√©lib√©r√©ment le contenu, m√™me celui explicitement marqu√© √† ignorer.

### Robots.txt
Le protocole d'exclusion des robots, plus connu sous le nom de robots.txt, est une convention visant √† emp√™cher les robots d'exploration (web crawlers) d'acc√©der √† tout ou une partie d'un site web.

Pour acc√©der √† ce fichier, il s'agit de taper : url_racine/robots.txt ou d'ajouter `/robots.txt` √† la fin de l'url racine d'un site que vous consultez.

Le fichier contient une liste de ressources du site qui ne sont pas cens√©es √™tre explor√©es par les moteurs de recherches.

Pour plus de d√©tails : [Robots.txt](https://robots-txt.com/)

## Sources et documentation

- [Tout sur le web scraping](https://kinsta.com/fr/base-de-connaissances/web-scraping/)

- [Comprendre l'environnement du scraping](https://datascientest.com/front-end-vs-back-end)

- [Qu'est-ce que le scraping de donn√©es](https://www.cloudflare.com/fr-fr/learning/bots/what-is-data-scraping/)

- [Web scraping : est-ce l√©gal](https://www.captaincontrat.com/protection-des-creations/cgv-cgu-cga/web-scraping-est-ce-legal-me-marcotte)

- [Le web scraping est-il l√©gal ?](https://www.iubenda.com/fr/help/111962-le-web-scraping-est-il-legal-ce-que-vous-devez-savoir#:~:text=La%20l%C3%A9galit%C3%A9%20du%20web%20scraping&text=Ne%20soyez%20pas%20trop%20enthousiaste,pas%20prot%C3%A9g%C3%A9es%20par%20un%20login)


- [Le cas Nestor (1)](https://www.alerionavocats.com/condamnation-societe-nestor-prospection-commerciale-fondee-interet-legitime-responsable-traitement-enseignements-tirer/)

- [Le cas Nestor (2)](https://www.plravocats.fr/blog/data-protection-rgpd/la-societe-nestor-sanctionee-par-la-cnil)

- [Pr√©sentation du fichier robots.txt](https://developers.google.com/search/docs/crawling-indexing/robots/intro?hl=fr)

- [Tableau de disponibilit√© Chrome](https://developer.chrome.com/docs/chromedriver/downloads?hl=fr)

- [Robots.txt](https://robots-txt.com/)

- [Choisir le navigateur](https://vedantkekan.medium.com/how-to-get-rid-of-choose-your-search-engine-dialog-in-chrome-v-127-on-selenium-test-run-ec46643a3228)

- [Eviter la d√©tection du BOT](https://www.zenrows.com/blog/selenium-avoid-bot-detection#how-anti-bots-work)

- [Documentation BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/)

- [Parsers BeautifuSoup](https://beautiful-soup-4.readthedocs.io/en/latest/index.html?highlight=parser#installing-a-parser)

- [Github de Selenium](https://github.com/SeleniumHQ)

- [Documentation Selenium](https://selenium-python.readthedocs.io/)

- [Documentation Selenium 4.22.0](https://www.selenium.dev/selenium/docs/api/py/api.html)


